{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title_cell",
   "metadata": {},
   "source": [
    "# Video Game Review Analysis: Critics vs. Users\n",
    "### A Statistical Study of Metacritic, IGN, and OpenCritic Trends\n",
    "\n",
    "This notebook explores whether professional game critics and general players share the same perspective on game quality. We look for \"review inflation\" over time and identify games with the largest sentiment gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec41b5",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "The loading of the dataset and initial setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loading_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel, levene, pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setting visualization style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "try:\n",
    "    ign = pd.read_csv('Databases/IGN_data.csv')\n",
    "    meta = pd.read_csv('metacritic_pc_games.csv')\n",
    "    oc = pd.read_csv('Opencritic_dataset.csv')\n",
    "except FileNotFoundError:\n",
    "    ign = pd.read_csv('IGN_data.csv')\n",
    "    meta = pd.read_csv('metacritic_pc_games.csv')\n",
    "    oc = pd.read_csv('Opencritic_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleaning_md",
   "metadata": {},
   "source": [
    "## 2. Preprocessing & Normalization\n",
    "standardize the column names and scale all scores to a **0-100** range to ensure fair comparisons between sources like IGN (0-10) and Metacritic (0-100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleaning_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [ign, meta, oc]:\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "# Standardizing Column Names\n",
    "meta.rename(columns={'Game Title':'game', 'Overall Metascore':'critic_score', \n",
    "                     'Overall User Rating':'user_score', 'Game Release Date':'date'}, inplace=True)\n",
    "oc.rename(columns={'Title':'game', 'Score':'critic_score_oc', 'Release Date':'date_oc'}, inplace=True)\n",
    "ign.rename(columns={'game':'game', 'score':'critic_score_ign', 'released_date':'date_ign'}, inplace=True)\n",
    "\n",
    "# Date Conversion\n",
    "meta['date'] = pd.to_datetime(meta['date'], errors='coerce')\n",
    "oc['date_oc'] = pd.to_datetime(oc['date_oc'], errors='coerce')\n",
    "ign['date_ign'] = pd.to_datetime(ign['date_ign'], errors='coerce')\n",
    "\n",
    "# Merging\n",
    "merged = meta.merge(oc, on='game', how='outer').merge(ign, on='game', how='outer')\n",
    "\n",
    "# Scaling 0-10 sources to 0-100\n",
    "for col in ['critic_score', 'user_score', 'critic_score_oc', 'critic_score_ign']:\n",
    "    merged[col] = pd.to_numeric(merged[col], errors='coerce')\n",
    "\n",
    "merged['user_score'] = merged['user_score'] * 10\n",
    "merged['critic_score_ign'] = merged['critic_score_ign'] * 10\n",
    "\n",
    "# Year consolidation\n",
    "merged['year'] = merged['date'].dt.year.fillna(merged['date_oc'].dt.year).fillna(merged['date_ign'].dt.year)\n",
    "merged_clean = merged.dropna(subset=['year']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats_md",
   "metadata": {},
   "source": [
    "## 3. Hypothesis Testing\n",
    "apply statistical tests to validate observations:\n",
    "* **Pearson Correlation**: To see if scores are rising/falling over the years.\n",
    "* **Paired T-Test**: To determine if the average difference between Critics and Users is statistically significant.\n",
    "* **Levene's Test**: To compare the variance (spread) of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "\n",
    "# H1: Metacritic Trend\n",
    "df_h1 = merged_clean.dropna(subset=['critic_score', 'year'])\n",
    "corr_meta, p_meta = pearsonr(df_h1['year'], df_h1['critic_score'])\n",
    "test_results.append({'Hypothesis': 'H1: Critics vs Year', 'Stat': corr_meta, 'P-Value': p_meta})\n",
    "\n",
    "# H2: Critic vs User Mean (Paired)\n",
    "df_h2 = merged_clean.dropna(subset=['critic_score', 'user_score'])\n",
    "t_stat, p_val_h2 = ttest_rel(df_h2['critic_score'], df_h2['user_score'])\n",
    "test_results.append({'Hypothesis': 'H2: Critic vs User Mean', 'Stat': t_stat, 'P-Value': p_val_h2})\n",
    "\n",
    "# H3: IGN Trend\n",
    "df_h3 = merged_clean.dropna(subset=['critic_score_ign', 'year'])\n",
    "corr_ign, p_ign = pearsonr(df_h3['year'], df_h3['critic_score_ign'])\n",
    "test_results.append({'Hypothesis': 'H3: IGN vs Year', 'Stat': corr_ign, 'P-Value': p_ign})\n",
    "\n",
    "# H4: Variance Comparison\n",
    "stat_var, p_val_var = levene(df_h2['user_score'], df_h2['critic_score'])\n",
    "test_results.append({'Hypothesis': 'H4: Variance (User vs Critic)', 'Stat': stat_var, 'P-Value': p_val_var})\n",
    "\n",
    "summary_df = pd.DataFrame(test_results)\n",
    "summary_df['Significant?'] = summary_df['P-Value'] < 0.05\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz_md",
   "metadata": {},
   "source": [
    "## 4. Visual Analysis\n",
    "Visualizing the distributions and trends discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Trend Over Time\n",
    "yearly_trends = merged_clean.groupby('year')[['critic_score', 'critic_score_ign']].mean()\n",
    "sns.lineplot(data=yearly_trends, ax=axes[0,0], palette=\"magma\", linewidth=2.5)\n",
    "axes[0,0].set_title('Average Review Scores Over Time (0-100 Scale)')\n",
    "\n",
    "# Distributions\n",
    "sns.kdeplot(merged_clean['critic_score'], label='Critics (Meta)', fill=True, ax=axes[0,1])\n",
    "sns.kdeplot(merged_clean['user_score'], label='Users (Meta)', fill=True, ax=axes[0,1])\n",
    "axes[0,1].set_title('Density of Review Scores')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Boxplot for Spread\n",
    "df_melted = df_h2.melt(id_vars=['game'], value_vars=['critic_score', 'user_score'], \n",
    "                        var_name='Type', value_name='Score')\n",
    "sns.boxplot(x='Type', y='Score', data=df_melted, ax=axes[1,1], palette=\"Set2\")\n",
    "axes[1,1].set_title('Spread of Scores: Critics vs. Users')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversy_md",
   "metadata": {},
   "source": [
    "## 5. Identifying Controversial Titles\n",
    "define controversy as the absolute difference between Critic scores and User scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversy_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_discrepancies_all_time(df, n=5):\n",
    "    paired = df.dropna(subset=['critic_score', 'user_score', 'game']).groupby('game', as_index=False).agg({\n",
    "        'critic_score': 'mean', \n",
    "        'user_score': 'mean'\n",
    "    })\n",
    "    paired['score_diff'] = paired['critic_score'] - paired['user_score']\n",
    "    paired['abs_diff'] = paired['score_diff'].abs()\n",
    "    return paired.sort_values('abs_diff', ascending=False).head(n)\n",
    "\n",
    "print(\"TOP 5 MOST CONTROVERSIAL GAMES OF ALL TIME\")\n",
    "top_discrepancies_all_time(merged_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
},
{
   "cell_type": "markdown",
   "id": "controversy_yearly_md",
   "metadata": {},
   "source": [
    "## 6. Annual Controversy Analysis\n",
    "Beyond general trends, we want to identify the specific titles that divided opinion each year. \n",
    "\n",
    "The function below calculates the **'Controversy Gap'**: \n",
    "1. It groups data by year and game.\n",
    "2. It calculates the absolute difference ($|Critic - User|$).\n",
    "3. It returns the top $N$ most disputed games for every year in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversy_yearly_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_discrepancies_per_year(\n",
    "    df,\n",
    "    n=5,\n",
    "    critic_col='critic_score',\n",
    "    user_col='user_score',\n",
    "    year_col='year'\n",
    "):\n",
    "    # 1. Clean and Group\n",
    "    paired = (\n",
    "        df\n",
    "        .dropna(subset=[critic_col, user_col, year_col, 'game'])\n",
    "        .groupby([year_col, 'game'], as_index=False)\n",
    "        .agg({\n",
    "            critic_col: 'mean',\n",
    "            user_col: 'mean'\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    # 2. Calculate Discrepancy\n",
    "    paired['score_diff'] = paired[critic_col] - paired[user_col]\n",
    "    paired['abs_diff'] = paired['score_diff'].abs()\n",
    "    \n",
    "    # 3. Sort and Filter top N per year\n",
    "    top_n = (\n",
    "        paired\n",
    "        .sort_values([year_col, 'abs_diff'], ascending=[True, False])\n",
    "        .groupby(year_col)\n",
    "        .head(n)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return top_n\n",
    "\n",
    "# Execute and Display\n",
    "yearly_controversy = top_discrepancies_per_year(merged_clean, n=3)\n",
    "print(\"MOST CONTROVERSIAL GAMES PER YEAR (Sample View):\")\n",
    "display(yearly_controversy.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_output_placeholder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Cell for user-defined analysis\n"
   ]
  }
