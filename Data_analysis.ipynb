{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_cell",
   "metadata": {},
   "source": [
    "# Video Game Review Analysis: Critics vs. Users\n",
    "### A Statistical Study of Metacritic, IGN, and OpenCritic Trends\n",
    "\n",
    "This notebook explores the relationship between professional critics and general players. We investigate \"review inflation,\" scoring biases, and identify the most controversial titles in gaming history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loading_md",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "Loading the datasets and initial environment setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel, levene, pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "try:\n",
    "    ign = pd.read_csv('Databases/IGN_data.csv')\n",
    "    meta = pd.read_csv('metacritic_pc_games.csv')\n",
    "    oc = pd.read_csv('Opencritic_dataset.csv')\n",
    "except FileNotFoundError:\n",
    "    ign = pd.read_csv('IGN_data.csv')\n",
    "    meta = pd.read_csv('metacritic_pc_games.csv')\n",
    "    oc = pd.read_csv('Opencritic_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleaning_md",
   "metadata": {},
   "source": [
    "## 2. Preprocessing & Normalization\n",
    "Standardizing column names and scaling all scores to a **0-100** range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processing_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [ign, meta, oc]:\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "meta.rename(columns={'Game Title':'game', 'Overall Metascore':'critic_score', \n",
    "                     'Overall User Rating':'user_score', 'Game Release Date':'date'}, inplace=True)\n",
    "oc.rename(columns={'Title':'game', 'Score':'critic_score_oc', 'Release Date':'date_oc'}, inplace=True)\n",
    "ign.rename(columns={'game':'game', 'score':'critic_score_ign', 'released_date':'date_ign'}, inplace=True)\n",
    "\n",
    "meta['date'] = pd.to_datetime(meta['date'], errors='coerce')\n",
    "oc['date_oc'] = pd.to_datetime(oc['date_oc'], errors='coerce')\n",
    "ign['date_ign'] = pd.to_datetime(ign['date_ign'], errors='coerce')\n",
    "\n",
    "merged = meta.merge(oc, on='game', how='outer').merge(ign, on='game', how='outer')\n",
    "\n",
    "for col in ['critic_score', 'user_score', 'critic_score_oc', 'critic_score_ign']:\n",
    "    merged[col] = pd.to_numeric(merged[col], errors='coerce')\n",
    "\n",
    "merged['user_score'] = merged['user_score'] * 10\n",
    "merged['critic_score_ign'] = merged['critic_score_ign'] * 10\n",
    "\n",
    "merged['year'] = merged['date'].dt.year.fillna(merged['date_oc'].dt.year).fillna(merged['date_ign'].dt.year)\n",
    "merged_clean = merged.dropna(subset=['year']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats_md",
   "metadata": {},
   "source": [
    "## 3. Hypothesis Testing\n",
    "Validating trends using Pearson's Correlation and checking for bias via Paired T-Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "testing_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "\n",
    "df_h1 = merged_clean.dropna(subset=['critic_score', 'year'])\n",
    "corr_meta, p_meta = pearsonr(df_h1['year'], df_h1['critic_score'])\n",
    "test_results.append({'Hypothesis': 'H1: Critics vs Year', 'Stat': corr_meta, 'P-Value': p_meta})\n",
    "\n",
    "df_h2 = merged_clean.dropna(subset=['critic_score', 'user_score'])\n",
    "t_stat, p_val_h2 = ttest_rel(df_h2['critic_score'], df_h2['user_score'])\n",
    "test_results.append({'Hypothesis': 'H2: Critic vs User Mean', 'Stat': t_stat, 'P-Value': p_val_h2})\n",
    "\n",
    "stat_var, p_val_var = levene(df_h2['user_score'], df_h2['critic_score'])\n",
    "test_results.append({'Hypothesis': 'H4: Variance Comparison', 'Stat': stat_var, 'P-Value': p_val_var})\n",
    "\n",
    "summary_df = pd.DataFrame(test_results)\n",
    "summary_df['Significant?'] = summary_df['P-Value'] < 0.05\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz_image_cell",
   "metadata": {},
   "source": [
    "## 4. Visual Analysis\n",
    "Below is the visualization of score distributions and yearly trends.\n",
    "\n",
    "![Review Scores Analysis Output](YOUR_IMAGE_PATH_OR_URL_HERE)\n",
    "\n",
    "*Note: If viewing locally, ensure the image file is in the same directory.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphs_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "sns.kdeplot(merged_clean['critic_score'], label='Critics (Meta)', fill=True, ax=axes[0])\n",
    "sns.kdeplot(merged_clean['user_score'], label='Users (Meta)', fill=True, ax=axes[0])\n",
    "axes[0].set_title('Density of Review Scores')\n",
    "axes[0].legend()\n",
    "\n",
    "yearly_trends = merged_clean.groupby('year')[['critic_score', 'user_score']].mean()\n",
    "sns.lineplot(data=yearly_trends, ax=axes[1])\n",
    "axes[1].set_title('Average Review Scores Over Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "func_md",
   "metadata": {},
   "source": [
    "## 5. Annual Controversy Function\n",
    "Identifying titles with the largest absolute gap between professional and public opinion per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "function_code",
   "metadata": {},
   "outputs": [
   "aaa",
   ],
   "source": [
    "def top_discrepancies_per_year(\n",
    "    df,\n",
    "    n=5,\n",
    "    critic_col='critic_score',\n",
    "    user_col='user_score',\n",
    "    year_col='year'\n",
    "):\n",
    "    paired = (\n",
    "        df\n",
    "        .dropna(subset=[critic_col, user_col, year_col, 'game'])\n",
    "        .groupby([year_col, 'game'], as_index=False)\n",
    "        .agg({\n",
    "            critic_col: 'mean',\n",
    "            user_col: 'mean'\n",
    "        })\n",
    "    )\n",
    "    paired['score_diff'] = paired[critic_col] - paired[user_col]\n",
    "    paired['abs_diff'] = paired['score_diff'].abs()\n",
    "    top_n = (\n",
    "        paired\n",
    "        .sort_values([year_col, 'abs_diff'], ascending=[True, False])\n",
    "        .groupby(year_col)\n",
    "        .head(n)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return top_n\n",
    "\n",
    "# Displaying result\n",
    "controversy_results = top_discrepancies_per_year(merged_clean, n=5)\n",
    "display(controversy_results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion_md",
   "metadata": {},
   "source": [
    "### Final Summary\n",
    "The data suggests a statistically significant gap between how critics and users perceive game value, often driven by extreme polarization in user reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": { "name": "ipython", "version": 3 },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
