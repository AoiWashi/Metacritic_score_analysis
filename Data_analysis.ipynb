{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_cell",
   "metadata": {},
   "source": [
    "# Video Game Review Analysis: Critics vs. Users\n",
    "### A Statistical Study of Metacritic, IGN, and OpenCritic Trends\n",
    "\n",
    "This notebook explores the relationship between professional critics and general players. We investigate \"review inflation,\" scoring biases, and identify the most controversial titles in gaming history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loading_md",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "The loading of the dataset and initial environment setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel, levene, pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "try:\n",
    "    ign = pd.read_csv('Databases/IGN_data.csv')\n",
    "    meta = pd.read_csv('metacritic_pc_games.csv')\n",
    "    oc = pd.read_csv('Opencritic_dataset.csv')\n",
    "except FileNotFoundError:\n",
    "    ign = pd.read_csv('IGN_data.csv')\n",
    "    meta = pd.read_csv('metacritic_pc_games.csv')\n",
    "    oc = pd.read_csv('Opencritic_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleaning_md",
   "metadata": {},
   "source": [
    "## 2. Preprocessing & Normalization\n",
    "Standardizing column names and scaling scores to a 0-100 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processing_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [ign, meta, oc]:\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "meta.rename(columns={'Game Title':'game', 'Overall Metascore':'critic_score', \n",
    "                     'Overall User Rating':'user_score', 'Game Release Date':'date'}, inplace=True)\n",
    "oc.rename(columns={'Title':'game', 'Score':'critic_score_oc', 'Release Date':'date_oc'}, inplace=True)\n",
    "ign.rename(columns={'game':'game', 'score':'critic_score_ign', 'released_date':'date_ign'}, inplace=True)\n",
    "\n",
    "meta['date'] = pd.to_datetime(meta['date'], errors='coerce')\n",
    "oc['date_oc'] = pd.to_datetime(oc['date_oc'], errors='coerce')\n",
    "ign['date_ign'] = pd.to_datetime(ign['date_ign'], errors='coerce')\n",
    "\n",
    "merged = meta.merge(oc, on='game', how='outer').merge(ign, on='game', how='outer')\n",
    "\n",
    "for col in ['critic_score', 'user_score', 'critic_score_oc', 'critic_score_ign']:\n",
    "    merged[col] = pd.to_numeric(merged[col], errors='coerce')\n",
    "\n",
    "merged['user_score'] = merged['user_score'] * 10\n",
    "merged['critic_score_ign'] = merged['critic_score_ign'] * 10\n",
    "\n",
    "merged['year'] = merged['date'].dt.year.fillna(merged['date_oc'].dt.year).fillna(merged['date_ign'].dt.year)\n",
    "merged_clean = merged.dropna(subset=['year']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats_md",
   "metadata": {},
   "source": [
    "## 3. Hypothesis Testing\n",
    "Validating trends using Pearson's Correlation and checking for bias via Paired T-Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "testing_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "\n",
    "df_h1 = merged_clean.dropna(subset=['critic_score', 'year'])\n",
    "corr_meta, p_meta = pearsonr(df_h1['year'], df_h1['critic_score'])\n",
    "test_results.append({'Hypothesis': 'H1: Critics vs Year', 'Stat': corr_meta, 'P-Value': p_meta})\n",
    "\n",
    "df_h2 = merged_clean.dropna(subset=['critic_score', 'user_score'])\n",
    "t_stat, p_val_h2 = ttest_rel(df_h2['critic_score'], df_h2['user_score'])\n",
    "test_results.append({'Hypothesis': 'H2: Critic vs User Mean', 'Stat': t_stat, 'P-Value': p_val_h2})\n",
    "\n",
    "stat_var, p_val_var = levene(df_h2['user_score'], df_h2['critic_score'])\n",
    "test_results.append({'Hypothesis': 'H4: Variance Comparison', 'Stat': stat_var, 'P-Value': p_val_var})\n",
    "\n",
    "summary_df = pd.DataFrame(test_results)\n",
    "summary_df['Significant?'] = summary_df['P-Value'] < 0.05\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz_md",
   "metadata": {},
   "source": [
    "## 4. Visualizing the Data\n",
    "Generating density plots and trend lines to visualize the Critic-User gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphs_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "sns.kdeplot(merged_clean['critic_score'], label='Critics (Meta)', fill=True, ax=axes[0])\n",
    "sns.kdeplot(merged_clean['user_score'], label='Users (Meta)', fill=True, ax=axes[0])\n",
    "axes[0].set_title('Density of Review Scores')\n",
    "axes[0].legend()\n",
    "\n",
    "yearly_trends = merged_clean.groupby('year')[['critic_score', 'user_score']].mean()\n",
    "sns.lineplot(data=yearly_trends, ax=axes[1])\n",
    "axes[1].set_title('Average Review Scores Over Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "func_md",
   "metadata": {},
   "source": [
    "## 5. Controversy Analysis\n",
    "Using a custom function to identify the top games with the highest discrepancy between groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "function_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_discrepancies_per_year(df, n=5):\n",
    "    paired = (\n",
    "        df.dropna(subset=['critic_score', 'user_score', 'year', 'game'])\n",
    "        .groupby(['year', 'game'], as_index=False)\n",
    "        .agg({'critic_score': 'mean', 'user_score': 'mean'})\n",
    "    )\n",
    "    paired['score_diff'] = paired['critic_score'] - paired['user_score']\n",
    "    paired['abs_diff'] = paired['score_diff'].abs()\n",
    "    \n",
    "    return (\n",
    "        paired.sort_values(['year', 'abs_diff'], ascending=[True, False])\n",
    "        .groupby('year').head(n)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "controversy_df = top_discrepancies_per_year(merged_clean)\n",
    "display(controversy_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion_md",
   "metadata": {},
   "source": [
    "### Final Summary\n",
    "**Key Findings:**\n",
    "1. **Bias Detection:** Professional critics consistently score games higher than users (statistically significant via T-Test).\n",
    "2. **Polarization:** Users exhibit significantly higher variance, indicating more frequent use of extreme scores (0 or 100) compared to critics.\n",
    "3. **Trends:** Review scores have remained relatively stable over the last decade, with minor fluctuations during major console releases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": { "name": "ipython", "version": 3 },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
